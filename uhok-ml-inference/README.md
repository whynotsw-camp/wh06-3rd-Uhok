# UHOK ML Inference Service

ë ˆì‹œí”¼ ì¶”ì²œì„ ìœ„í•œ ì„ë² ë”© ìƒì„± ML ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ğŸ¯ ëª©ì 

- **ë¹„ìš© ìµœì í™”**: ë¬´ê±°ìš´ ML ëª¨ë¸ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬ EC2 ë¹„ìš© ì ˆì•½
- **í™•ì¥ì„±**: ML ì„œë¹„ìŠ¤ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥
- **ìœ ì§€ë³´ìˆ˜ì„±**: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”

## ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ

- **FastAPI**: ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ì›¹ í”„ë ˆì„ì›Œí¬
- **SentenceTransformers**: ë¬¸ì¥ ì„ë² ë”© ìƒì„± (paraphrase-multilingual-MiniLM-L12-v2)
- **PyTorch**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ (CPU ì „ìš©)
- **Docker**: ì»¨í…Œì´ë„ˆí™”ëœ ë°°í¬

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
[Backend Service] --HTTP--> [ML Inference Service]
     â†“                              â†“
[PostgreSQL]                 [SentenceTransformer]
[pgvector]                   [paraphrase-multilingual-MiniLM-L12-v2]
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### Dockerë¡œ ì‹¤í–‰
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t uhok-ml-inference .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8001:8001 uhok-ml-inference
```

### Docker Composeë¡œ ì‹¤í–‰ (ê¶Œì¥)
```bash
# uhok-deployì˜ ml í´ë”ì—ì„œ ì‹¤í–‰
cd uhok-deploy/ml
docker-compose -f docker-compose.ml.yml up -d
```

### Makefile ì‚¬ìš© (uhok-deploy í†µí•©)
```bash
# uhok-deployì˜ public í´ë”ì—ì„œ ì‹¤í–‰
cd uhok-deploy/public
make up-ml
```

## ğŸ“¡ API ì‚¬ìš©ë²•

### í—¬ìŠ¤ ì²´í¬
```bash
curl http://localhost:8001/health
```

### ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
```bash
curl -X POST http://localhost:8001/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "ê°ˆë¹„íƒ•", "normalize": true}'
```

### ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
```bash
curl -X POST http://localhost:8001/api/v1/embed-batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["ê°ˆë¹„íƒ•", "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ"], "normalize": true}'
```

## ğŸ”§ ê°œë°œ í™˜ê²½ ì„¤ì •

### ë¡œì»¬ ê°œë°œ
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œ ì„œë²„ ì‹¤í–‰
python -m app.main
```

### Docker Compose ì‚¬ìš© (ê¶Œì¥)
```bash
# ML ì„œë¹„ìŠ¤ ì‹¤í–‰
cd uhok-deploy/ml
docker-compose -f docker-compose.ml.yml up --build

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
docker-compose -f docker-compose.ml.yml up -d

# ë¡œê·¸ í™•ì¸
docker-compose -f docker-compose.ml.yml logs -f

# ì„œë¹„ìŠ¤ ì¤‘ì§€
docker-compose -f docker-compose.ml.yml down
```

### Makefile ì‚¬ìš© (í†µí•© í™˜ê²½)
```bash
# uhok-deployì˜ public í´ë”ì—ì„œ ì‹¤í–‰
cd uhok-deploy/public

# ML ì„œë¹„ìŠ¤ ì‹¤í–‰
make up-ml

# ML ì„œë¹„ìŠ¤ ì¬ì‹œì‘
make restart-ml

# ML ì„œë¹„ìŠ¤ ë¡œê·¸ í™•ì¸
make logs-ml

# ML ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
make status-ml
```

## ğŸ“Š ì„±ëŠ¥ íŠ¹ì„±

- **ëª¨ë¸**: paraphrase-multilingual-MiniLM-L12-v2 (384ì°¨ì›)
- **ì²˜ë¦¬ëŸ‰**: CPU ê¸°ë°˜, ë‹¨ì¼ ì›Œì»¤
- **ì§€ì—°ì‹œê°„**: ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë”© ì‹œê°„ í¬í•¨
- **ë©”ëª¨ë¦¬**: ì•½ 1-2GB (ëª¨ë¸ + ëŸ°íƒ€ì„)

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ê¸°ë³¸ í…ŒìŠ¤íŠ¸
```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8001/health

# ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "ê°ˆë¹„íƒ•", "normalize": true}'

# ë°°ì¹˜ ì„ë² ë”© í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/api/v1/embed-batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["ê°ˆë¹„íƒ•", "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ"], "normalize": true}'

# ëª¨ë¸ ì •ë³´ ì¡°íšŒ
curl http://localhost:8001/api/v1/model-info
```

### í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```bash
# uhok-deploy/ml ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
cd uhok-deploy/ml
python test_ml_integration.py
```

### Makefileì„ í†µí•œ í…ŒìŠ¤íŠ¸
```bash
# uhok-deployì˜ public í´ë”ì—ì„œ ì‹¤í–‰
cd uhok-deploy/public

# ML ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
make status-ml

# ML ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
curl http://localhost:8001/health
```

## ğŸ”„ ë°±ì—”ë“œ ì—°ë™

ë°±ì—”ë“œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì›ê²© ML ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤:

```python
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ML_INFERENCE_URL=http://ml-inference:8001
ML_TIMEOUT=30.0  # ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤í•˜ì—¬ ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
ML_RETRIES=2

# ì›ê²© ì„ë² ë”© í˜¸ì¶œ
async with httpx.AsyncClient(timeout=ML_TIMEOUT) as client:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/embed",
        json={"text": query, "normalize": True}
    )
    response.raise_for_status()
    result = response.json()
    embedding = result["embedding"]
    dim = result["dim"]
```

### ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/embed",
        json={"text": query, "normalize": True}
    )
    response.raise_for_status()
    return response.json()["embedding"]
except httpx.TimeoutException:
    logger.error("ML ì„œë¹„ìŠ¤ íƒ€ì„ì•„ì›ƒ")
    return None
except httpx.HTTPStatusError as e:
    logger.error(f"ML ì„œë¹„ìŠ¤ HTTP ì—ëŸ¬: {e.response.status_code}")
    return None
except Exception as e:
    logger.error(f"ML ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    return None
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸
```bash
# Docker Composeë¡œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
cd uhok-deploy/ml
docker-compose -f docker-compose.ml.yml logs -f

# Makefile ì‚¬ìš© (ê¶Œì¥)
cd uhok-deploy/public
make logs-ml

# ì§ì ‘ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
python -m app.main
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats uhok-ml-inference

# CPU ì‚¬ìš©ëŸ‰ í™•ì¸
docker exec uhok-ml-inference top

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
cd uhok-deploy/public
make status-ml
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **ì²« ìš”ì²­ ì§€ì—°**: ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì¸í•œ ì½œë“œìŠ¤íƒ€íŠ¸ (ì•½ 10-30ì´ˆ)
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•œ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš© (1-2GB)
3. **ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„±**: ë°±ì—”ë“œì™€ ML ì„œë¹„ìŠ¤ ê°„ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í•„ìš”
4. **ì—ëŸ¬ ì²˜ë¦¬**: ML ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ í•„ìš”
5. **íƒ€ì„ì•„ì›ƒ ì„¤ì •**: ëª¨ë¸ ë¡œë”© ì‹œê°„ì„ ê³ ë ¤í•œ ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì • í•„ìš”
6. **ë™ì‹œì„±**: ë‹¨ì¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¸í•œ ì²˜ë¦¬ëŸ‰ ì œí•œ

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ìºì‹œ ê´€ë¦¬
```bash
# HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
ls -la ~/.cache/huggingface/

# ìºì‹œ ì •ë¦¬ (í•„ìš”ì‹œ)
rm -rf ~/.cache/huggingface/
```

### ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
```bash
# ì—°ê²° í…ŒìŠ¤íŠ¸
curl http://localhost:8001/health

# í¬íŠ¸ í™•ì¸
telnet localhost 8001
```

### ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²°
```bash
# ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
docker run -m 4g -p 8001:8001 uhok-ml-inference

# ë˜ëŠ” uhok-deploy/ml/docker-compose.ml.ymlì—ì„œ
services:
  ml-inference:
    deploy:
      resources:
        limits:
          memory: 4G
```

## ğŸ”„ ë²„ì „ ê´€ë¦¬

### ë²„ì „ ì—…ê·¸ë ˆì´ë“œ
```bash
# 1. uhok-deploy/ml/docker-compose.ml.ymlì—ì„œ ì´ë¯¸ì§€ ë²„ì „ ìˆ˜ì •
# image: uhok-ml-inference:1.0.1 â†’ uhok-ml-inference:1.0.2

# 2. ìƒˆ ì´ë¯¸ì§€ ë¹Œë“œ ë° ì¬ì‹œì‘
cd uhok-deploy/ml
docker-compose -f docker-compose.ml.yml build --no-cache
docker-compose -f docker-compose.ml.yml down
docker-compose -f docker-compose.ml.yml up -d

# ë˜ëŠ” Makefile ì‚¬ìš©
cd uhok-deploy/public
make restart-ml
```

### ë¡¤ë°±
```bash
# ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
# uhok-deploy/ml/docker-compose.ml.ymlì—ì„œ ì´ì „ ë²„ì „ìœ¼ë¡œ ìˆ˜ì • í›„
cd uhok-deploy/ml
docker-compose -f docker-compose.ml.yml down
docker-compose -f docker-compose.ml.yml up -d

# ë˜ëŠ” Makefile ì‚¬ìš©
cd uhok-deploy/public
make restart-ml
```

## ğŸ“š API ë¬¸ì„œ

ìì„¸í•œ API ë¬¸ì„œëŠ” ì„œë¹„ìŠ¤ ì‹¤í–‰ í›„ ë‹¤ìŒ URLì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `LICENSE` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´:
1. ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”
2. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: `cd uhok-deploy/public && make logs-ml`
3. í—¬ìŠ¤ì²´í¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: `curl http://localhost:8001/health`
4. ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: `cd uhok-deploy/public && make status-ml`