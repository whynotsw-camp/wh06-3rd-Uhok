# ML ì„œë¹„ìŠ¤ í†µí•© ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

ì´ ê°€ì´ë“œëŠ” ë°±ì—”ë“œì—ì„œ ë¬´ê±°ìš´ ML ëª¨ë¸ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬ EC2 ë¹„ìš©ì„ ì ˆì•½í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ êµ¬í˜„ ì™„ë£Œ ì‚¬í•­

### âœ… ML Inference ì„œë¹„ìŠ¤
- **ìœ„ì¹˜**: `uhok-ml-inference/`
- **ê¸°ìˆ **: FastAPI + SentenceTransformer
- **ê¸°ëŠ¥**: í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± API
- **ëª¨ë¸**: paraphrase-multilingual-MiniLM-L12-v2 (384ì°¨ì›)

### âœ… ë°±ì—”ë“œ ì–´ëŒ‘í„°
- **ìœ„ì¹˜**: `uhok-backend/services/recipe/utils/remote_ml_adapter.py`
- **ê¸°ëŠ¥**: ì›ê²© ML ì„œë¹„ìŠ¤ í˜¸ì¶œ ë° PostgreSQL pgvector ê²€ìƒ‰
- **í™˜ê²½ë³€ìˆ˜**: `ML_MODE`ë¡œ ë¡œì»¬/ì›ê²© ëª¨ë“œ ì „í™˜

### âœ… Docker í†µí•©
- **ìœ„ì¹˜**: `uhok-deploy/docker-compose.yml`
- **ì„œë¹„ìŠ¤**: `ml-inference` (í”„ë¡œí•„: `with-ml`)
- **ë„¤íŠ¸ì›Œí¬**: ë‚´ë¶€ í†µì‹ ë§Œ í—ˆìš© (ë³´ì•ˆ)

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ê°œë°œ í™˜ê²½ (ë¡œì»¬ ëª¨ë“œ)
```bash
# ë°±ì—”ë“œë§Œ ì‹¤í–‰ (ê¸°ì¡´ ë°©ì‹)
cd uhok-deploy
docker-compose up -d

# ë°±ì—”ë“œ í™˜ê²½ë³€ìˆ˜: ML_MODE=local (ê¸°ë³¸ê°’)
```

### 2. ìš´ì˜ í™˜ê²½ (ì›ê²© ëª¨ë“œ)
```bash
# ML ì„œë¹„ìŠ¤ í¬í•¨ ì‹¤í–‰
cd uhok-deploy
docker-compose --profile with-ml up -d

# ë°±ì—”ë“œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo "ML_MODE=remote_embed" >> uhok-backend/.env
echo "ML_INFERENCE_URL=http://ml-inference:8001" >> uhok-backend/.env
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. ML ì„œë¹„ìŠ¤ ë‹¨ë… í…ŒìŠ¤íŠ¸
```bash
# ML ì„œë¹„ìŠ¤ ë¹Œë“œ ë° ì‹¤í–‰
cd uhok-ml-inference
docker build -t uhok-ml-inference .
docker run -p 8001:8001 uhok-ml-inference

# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python test_ml_service.py
```

### 2. ë°±ì—”ë“œ ì—°ë™ í…ŒìŠ¤íŠ¸
```bash
# ML ì„œë¹„ìŠ¤ ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
cd uhok-ml-inference
docker run -p 8001:8001 uhok-ml-inference

# ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸
cd uhok-backend
python test_remote_ml.py
```

### 3. í†µí•© í…ŒìŠ¤íŠ¸
```bash
# ì „ì²´ ì„œë¹„ìŠ¤ ì‹¤í–‰
cd uhok-deploy
docker-compose --profile with-ml up -d

# í—¬ìŠ¤ì²´í¬
curl http://localhost:5000/api/health  # ë°±ì—”ë“œ
curl http://localhost:8001/health      # ML ì„œë¹„ìŠ¤ (í¬íŠ¸ í¬ì›Œë”© í•„ìš”)
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **ê¸°ì¡´ (ë¡œì»¬ ëª¨ë“œ)**: ë°±ì—”ë“œ 2-3GB (ëª¨ë¸ í¬í•¨)
- **ë¶„ë¦¬ í›„**: ë°±ì—”ë“œ 500MB, ML ì„œë¹„ìŠ¤ 1-2GB
- **ì ˆì•½**: ë°±ì—”ë“œ ë©”ëª¨ë¦¬ 50-70% ê°ì†Œ

### ì‘ë‹µ ì‹œê°„
- **ë¡œì»¬ ëª¨ë“œ**: ì²« ìš”ì²­ 10-30ì´ˆ (ëª¨ë¸ ë¡œë”©), ì´í›„ 100-300ms
- **ì›ê²© ëª¨ë“œ**: ì²« ìš”ì²­ 10-30ì´ˆ (ëª¨ë¸ ë¡œë”©), ì´í›„ 200-500ms (ë„¤íŠ¸ì›Œí¬ í¬í•¨)

### ë¹„ìš© ì ˆì•½
- **EC2 ì¸ìŠ¤í„´ìŠ¤**: ë°±ì—”ë“œìš© ë” ì‘ì€ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ê°€ëŠ¥
- **ìŠ¤ì¼€ì¼ë§**: ML ì„œë¹„ìŠ¤ ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§
- **ë°°í¬**: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ë°±ì—”ë“œ ì˜í–¥ ìµœì†Œí™”

## ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

### ë°±ì—”ë“œ (.env)
```bash
# ML ì„œë¹„ìŠ¤ ëª¨ë“œ ì„¤ì •
ML_MODE=remote_embed              # local, remote_embed, remote_store

# ì›ê²© ML ì„œë¹„ìŠ¤ ì„¤ì • (remote_* ëª¨ë“œì—ì„œ ì‚¬ìš©)
ML_INFERENCE_URL=http://ml-inference:8001
ML_TIMEOUT=3.0
ML_RETRIES=2
```

### ML ì„œë¹„ìŠ¤ (Docker í™˜ê²½ë³€ìˆ˜)
```bash
HF_HOME=/models/hf_cache          # ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
PYTHONUNBUFFERED=1               # ë¡œê·¸ ì‹¤ì‹œê°„ ì¶œë ¥
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­

### 1. ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„±
- ML ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ ë°±ì—”ë“œ ì¶”ì²œ ê¸°ëŠ¥ ì¤‘ë‹¨
- **í•´ê²°ì±…**: íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„, í´ë°± ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„

### 2. ëª¨ë¸ ë¡œë”© ì‹œê°„
- ì²« ìš”ì²­ ì‹œ 10-30ì´ˆ ì§€ì—° (ì½œë“œìŠ¤íƒ€íŠ¸)
- **í•´ê²°ì±…**: í—¬ìŠ¤ì²´í¬ë¡œ ì›Œë°ì—…, ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”©

### 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ML ì„œë¹„ìŠ¤: 1-2GB ë©”ëª¨ë¦¬ í•„ìš”
- **í•´ê²°ì±…**: ì ì ˆí•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ

### 4. ë³´ì•ˆ
- ML ì„œë¹„ìŠ¤ëŠ” ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ì—ì„œë§Œ ì ‘ê·¼
- **í•´ê²°ì±…**: VPC, ë³´ì•ˆ ê·¸ë£¹ ì„¤ì •

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸
```bash
# ML ì„œë¹„ìŠ¤ ë¡œê·¸
docker-compose logs -f ml-inference

# ë°±ì—”ë“œ ML í˜¸ì¶œ ë¡œê·¸
docker-compose logs -f backend | grep -i "ml\|embedding\|remote"
```

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ML ì„œë¹„ìŠ¤: ìš”ì²­ ìˆ˜, ì‘ë‹µ ì‹œê°„, ì—ëŸ¬ìœ¨
- ë°±ì—”ë“œ: ML í˜¸ì¶œ ì„±ê³µ/ì‹¤íŒ¨ìœ¨, íƒ€ì„ì•„ì›ƒ ë°œìƒë¥ 

### ì•Œë¦¼ ì„¤ì •
- ML ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨
- ì‘ë‹µ ì‹œê°„ ì„ê³„ì¹˜ ì´ˆê³¼
- ì—ëŸ¬ìœ¨ ì„ê³„ì¹˜ ì´ˆê³¼

## ğŸ”„ ë¡¤ë°± ê³„íš

### ê¸´ê¸‰ ë¡¤ë°±
```bash
# í™˜ê²½ë³€ìˆ˜ë§Œ ë³€ê²½í•˜ì—¬ ì¦‰ì‹œ ë¡¤ë°±
echo "ML_MODE=local" > uhok-backend/.env
docker-compose restart backend
```

### ì ì§„ì  ì „í™˜
1. **Shadow ëª¨ë“œ**: ê¸°ì¡´/ì‹ ê·œ ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë¹„êµ
2. **ë¶€ë¶„ íŠ¸ë˜í”½**: 10% â†’ 50% â†’ 100% ì ì°¨ ì „í™˜
3. **ìë™ í´ë°±**: ì—ëŸ¬ìœ¨/ì§€ì—° ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ìë™ ë¡¤ë°±

## ğŸ‰ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] ML Inference ì„œë¹„ìŠ¤ êµ¬í˜„
- [x] ë°±ì—”ë“œ ì›ê²© ì–´ëŒ‘í„° êµ¬í˜„
- [x] Docker Compose í†µí•©
- [x] í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
- [x] í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [x] ë¬¸ì„œí™” ì™„ë£Œ

## ğŸ“ ì§€ì›

ë¬¸ì œ ë°œìƒ ì‹œ:
1. ë¡œê·¸ í™•ì¸: `docker-compose logs -f ml-inference`
2. í—¬ìŠ¤ì²´í¬: `curl http://localhost:8001/health`
3. ë„¤íŠ¸ì›Œí¬ í™•ì¸: `docker-compose exec backend ping ml-inference`
4. í™˜ê²½ë³€ìˆ˜ í™•ì¸: `docker-compose exec backend env | grep ML_`
