# 분리 전략 먼저 결정 (A/B)

1. 전략 A – Remote Embed + DB pgvector 유지(권장)

- ML 서버: 임베딩만 생성 (/v1/embed)

- 백엔드: 받은 임베딩으로 PostgreSQL(pgvector) <-> 실행

- 장점: 기존 DB/인덱스/튜닝 그대로 활용, 데이터 이관 최소

2. 전략 B – Remote Vector Store

- ML 서버: 자체 인덱스(FAISS/HNSW 등) 보유, Top-K ID 직접 반환 (/v1/search)

- 백엔드: 받은 ID로 MariaDB 상세 조인

- 장점: 검색 성능 독립/확장 용이, 단점: 인덱스 동기화/피드 파이프라인 필요

---

## 단계별 진행 순서 (체크리스트)

### 0. 사전 정리

- 어떤 전략(A/B)을 쓸지 결정

- 모델/차원/토크나이저 버전 고정(예: paraphrase-multilingual-MiniLM-L12-v2, 384d)

- 운영 목표치: p95 지연, QPS, 예산, 롤백 기준

### 1. 새 리포 생성 (예: uhok-ml-inference)

- 기본 구조
```text
uhok-ml-inference/
├─ app/              # FastAPI/uvicorn 진입
│  ├─ main.py
│  ├─ api.py         # /health, /v1/embed, (/v1/search: 전략 B)
│  └─ deps.py        # 모델/리소스 로딩
├─ models/
│  └─ sentence_transformers.py  # 모델 로더/캐시
├─ store/            # (전략 B) 인덱스/메타
├─ Dockerfile
├─ requirements.txt
└─ README.md
```

- 요구 패키지: fastapi, uvicorn, sentence-transformers, torch, (전략 B면 faiss-cpu/nmslib 등)

### 2. API 스펙 정의 (변하지 않는 계약 먼저)

- 공통

    - GET /health → {status:"ok", model:"...", dim:384, version:"..."}

- 전략 A

    - POST /v1/embed → 요청: { "text": "갈비탕", "normalize": true }
      응답: { "embedding": [float, ...], "dim": 384, "version": "sbert-..." }

    - (배치) POST /v1/embed-batch → { "texts": [...], "normalize": true }

- 전략 B

    - POST /v1/search → { "query": "갈비탕", "top_k": 25, "exclude_ids": [..] }
      응답: { "results": [ { "recipe_id":123, "distance":0.21 }, ... ] }

    - (선택) POST /v1/upsert → 벡터/메타 업데이트, 인덱스 관리

이 스펙을 OpenAPI(yaml) 로 먼저 고정해두면, 백엔드/ML 팀이 병렬 작업 가능.

### 3. ML 서비스 구현

- 모델 로더: 프로세스 시작 시 1회 로딩(+ warmup) / 디바이스(CPU/GPU) 선택

- 엔드포인트: 위 API 스펙대로 구현, 타임아웃/에러 핸들링/입력 검증

- 옵셔널 고급: 미니 배칭, 레이트 리밋, 캐시(LRU), 텍스트 전처리(정규화/언어 감지)

### 4) 컨테이너화 & 실행

- Dockerfile 작성 → 이미지 빌드

- 로컬/개발: docker run -p 8080:8080 uhok-ml-inference

- 성능 확인: 동시성 옵션(--workers, --threads) 튜닝

### 5) 인프라 배치

- VPC 내부(사설망) 배치 권장: ALB/NLB(내부), SG로 백엔드 IP만 허용

- 배포 대상: EC2/ECS/EKS 중 택1

- 헬스체크: /health 200 OK

- 관측: 로그(JSON), 메트릭(p95/오류율), 트레이싱(필요 시)

### 6) 백엔드 연결(포트 교체)

- .env 스위치 추가
```ini
ML_MODE=remote_embed      # A: 원격 임베딩 + DB pgvector
# 또는
ML_MODE=remote_store      # B: 원격 검색(Top-K)
ML_INFERENCE_URL=http://uhok-ml-inference:8080
ML_TIMEOUT=3.0
ML_RETRIES=2
```

- ports 기반 어댑터 구현(백엔드):

    - A: RemoteDBVectorRecommender (임베딩만 원격 호출 → pgvector <->)

    - B: RemoteVectorStoreRecommender (Top-K ID 원격 반환)

- 팩토리에서 ML_MODE 보고 구현체 선택 → 라우터/CRUD는 무변경

### 7) 데이터/인덱스 준비

- 전략 A: 기존 RECIPE_VECTOR_TABLE/인덱스 유지(끝)

- 전략 B:

    - 초기 인덱스 빌드: 기존 pgvector에서 벡터 export → ML 인덱스 upsert

    - 파이프라인 수정: 신규/수정 레시피 시 ML 인덱스도 동기화

    - embedding_version 필드 도입(모델 변경 시 리빌드 판단)

### 8) 점진적 전환(릴리즈 플랜)

- Shadow 모드(권장): 요청은 기존/신규 모두 실행 → 응답 비교/로그 적재

- 부분 트래픽(예: 10% → 50% → 100%) 점차 전환

- 에러/지연 임계치 넘으면 자동 폴백(Local or DB)

### 9) 테스트 & 검증

- 단위: 어댑터(포트) 모킹/계약 테스트

- 통합: 엔드투엔드 API, exclude_ids, 페이지네이션

- 부하: p95, 에러율, 스로틀링 동작

### 10) 문서 & 운영

- API 계약(OpenAPI), 배포/롤백 절차, 버전 호환 정책

- 알림: 장애/지연 임계치, 재시도/서킷브레이커 설정

---

## (선택) 원격 어댑터 뼈대 — 전략 A/B 예시

필요시 백엔드에 추가. (요청 스타일에 맞춰 async 함수 + """주석""" 포함)

```python
# services/recommend/recommend_service_remote.py

import httpx
from typing import List, Tuple, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
from services.recommend.ports import VectorSearcherPort
from common.logger import get_logger
import os

logger = get_logger("recommend_remote")

class RemoteDBVectorRecommender(VectorSearcherPort):
    async def find_similar_ids(
        self,
        pg_db: AsyncSession,
        query: str,
        top_k: int,
        exclude_ids: Optional[List[int]] = None,
    ) -> List[Tuple[int, float]]:
        """
        원격 ML 서버에서 쿼리 임베딩을 받고, 로컬 PostgreSQL(pgvector)의 `<->`로 유사 Top-K를 조회한다.
        - /v1/embed 호출 → qv 수신 → DB에서 distance ASC
        - 반환: (recipe_id, distance) 리스트
        """
        base = os.getenv("ML_INFERENCE_URL", "http://uhok-ml-inference:8080")
        timeout = float(os.getenv("ML_TIMEOUT", "3.0"))
        async with httpx.AsyncClient(timeout=timeout) as client:
            resp = await client.post(f"{base}/v1/embed", json={"text": query, "normalize": True})
            resp.raise_for_status()
            qv = resp.json().get("embedding")

        if exclude_ids:
            placeholders = ",".join([f":ex{i}" for i in range(len(exclude_ids))])
            where_not_in = f'WHERE "RECIPE_ID" NOT IN ({placeholders})'
        else:
            where_not_in = ""

        sql = text(f'''
            SELECT "RECIPE_ID", "VECTOR_NAME" <-> :qv AS distance
            FROM "RECIPE_VECTOR_TABLE"
            {where_not_in}
            ORDER BY distance ASC
            LIMIT :k
        ''')
        params = {"qv": qv, "k": int(top_k)}
        if exclude_ids:
            for i, rid in enumerate(exclude_ids):
                params[f"ex{i}"] = int(rid)

        rows = (await pg_db.execute(sql, params)).all()
        return [(int(rid), float(dist)) for rid, dist in rows]


class RemoteVectorStoreRecommender(VectorSearcherPort):
    async def find_similar_ids(
        self,
        pg_db: AsyncSession,  # 미사용, 시그니처 유지
        query: str,
        top_k: int,
        exclude_ids: Optional[List[int]] = None,
    ) -> List[Tuple[int, float]]:
        """
        원격 ML Vector Store에서 Top-K 검색을 수행한다.
        - /v1/search 호출 → {results:[{recipe_id, distance}]}
        - 반환: (recipe_id, distance) 리스트
        """
        base = os.getenv("ML_INFERENCE_URL", "http://uhok-ml-inference:8080")
        timeout = float(os.getenv("ML_TIMEOUT", "3.0"))
        payload = {"query": query, "top_k": top_k, "exclude_ids": exclude_ids or []}
        async with httpx.AsyncClient(timeout=timeout) as client:
            resp = await client.post(f"{base}/v1/search", json=payload)
            resp.raise_for_status()
            data = resp.json().get("results", [])
        return [(int(x["recipe_id"]), float(x["distance"])) for x in data]
```
팩토리에서 ML_MODE에 따라 get_db_vector_searcher()가 Local(DBVectorRecommender) ↔ Remote(위 클래스) 를 선택하면 끝.

---

## 롤백 플랜(요약)

- .env의 ML_MODE만 local 또는 db로 즉시 전환 → 재배포 없이 재기동

- 장애 지표 임계치 초과 시 자동 폴백(서킷 브레이커 + 리트라이나 캐시)
